{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]:Char_RNN Example...\n",
      "[Author]:Charles\n",
      "[Usage]:python3 Char_RNN.py \n",
      "Enter 1 to train, enter 0 to gen...\n",
      "[Choice]:Train<1> or Gen<0>:1\n",
      "INFO:tensorflow:Summary name optimize/gradients/loss/MatMul_grad/MatMul_1:0 is illegal; using optimize/gradients/loss/MatMul_grad/MatMul_1_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/gradients/loss/add_grad/Reshape_1:0 is illegal; using optimize/gradients/loss/add_grad/Reshape_1_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/gradients/model/rnnlm/embedding_lookup_grad/Reshape:0 is illegal; using optimize/gradients/model/rnnlm/embedding_lookup_grad/Reshape_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/gradients/model/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul/Enter_grad/b_acc_3:0 is illegal; using optimize/gradients/model/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul/Enter_grad/b_acc_3_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/gradients/model/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/BiasAdd/Enter_grad/b_acc_3:0 is illegal; using optimize/gradients/model/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/BiasAdd/Enter_grad/b_acc_3_0 instead.\n",
      "[Step]:0/46625, [Train_loss]:157.555969\n",
      "[Step]:10/46625, [Train_loss]:150.058456\n",
      "[Step]:20/46625, [Train_loss]:138.902740\n",
      "[Step]:30/46625, [Train_loss]:141.348358\n",
      "[Step]:40/46625, [Train_loss]:127.939400\n",
      "[Step]:50/46625, [Train_loss]:134.169464\n",
      "[Step]:60/46625, [Train_loss]:132.660431\n",
      "[Step]:70/46625, [Train_loss]:137.191406\n",
      "[Step]:80/46625, [Train_loss]:130.441559\n",
      "[Step]:90/46625, [Train_loss]:145.927429\n",
      "[Step]:100/46625, [Train_loss]:127.402115\n",
      "[Step]:110/46625, [Train_loss]:131.504120\n",
      "[Step]:120/46625, [Train_loss]:123.544189\n",
      "[Step]:130/46625, [Train_loss]:125.729614\n",
      "[Step]:140/46625, [Train_loss]:119.194778\n",
      "[Step]:150/46625, [Train_loss]:141.488678\n",
      "[Step]:160/46625, [Train_loss]:125.651489\n",
      "[Step]:170/46625, [Train_loss]:123.376526\n",
      "[Step]:180/46625, [Train_loss]:120.643036\n",
      "[Step]:190/46625, [Train_loss]:122.162117\n",
      "[Step]:200/46625, [Train_loss]:118.899780\n",
      "[Step]:210/46625, [Train_loss]:118.090256\n",
      "[Step]:220/46625, [Train_loss]:127.865028\n",
      "[Step]:230/46625, [Train_loss]:125.420403\n",
      "[Step]:240/46625, [Train_loss]:129.920410\n",
      "[Step]:250/46625, [Train_loss]:124.550423\n",
      "[Step]:260/46625, [Train_loss]:123.500015\n",
      "[Step]:270/46625, [Train_loss]:127.725662\n",
      "[Step]:280/46625, [Train_loss]:117.692322\n",
      "[Step]:290/46625, [Train_loss]:128.214584\n",
      "[Step]:300/46625, [Train_loss]:128.394287\n",
      "[Step]:310/46625, [Train_loss]:129.061768\n",
      "[Step]:320/46625, [Train_loss]:120.373947\n",
      "[Step]:330/46625, [Train_loss]:128.798294\n",
      "[Step]:340/46625, [Train_loss]:124.874512\n",
      "[Step]:350/46625, [Train_loss]:135.185486\n",
      "[Step]:360/46625, [Train_loss]:123.384079\n",
      "[Step]:370/46625, [Train_loss]:128.815033\n",
      "[Step]:380/46625, [Train_loss]:118.647339\n",
      "[Step]:390/46625, [Train_loss]:126.685471\n",
      "[Step]:400/46625, [Train_loss]:114.769226\n",
      "[Step]:410/46625, [Train_loss]:118.032532\n",
      "[Step]:420/46625, [Train_loss]:117.910248\n",
      "[Step]:430/46625, [Train_loss]:132.304108\n",
      "[Step]:440/46625, [Train_loss]:123.221436\n",
      "[Step]:450/46625, [Train_loss]:115.637863\n",
      "[Step]:460/46625, [Train_loss]:116.559158\n",
      "[Step]:470/46625, [Train_loss]:116.825821\n",
      "[Step]:480/46625, [Train_loss]:115.967888\n",
      "[Step]:490/46625, [Train_loss]:106.542282\n",
      "[Step]:500/46625, [Train_loss]:122.466949\n",
      "[Step]:510/46625, [Train_loss]:121.770844\n",
      "[Step]:520/46625, [Train_loss]:124.304909\n",
      "[Step]:530/46625, [Train_loss]:121.719505\n",
      "[Step]:540/46625, [Train_loss]:120.924492\n",
      "[Step]:550/46625, [Train_loss]:122.010475\n",
      "[Step]:560/46625, [Train_loss]:111.840561\n",
      "[Step]:570/46625, [Train_loss]:120.334938\n",
      "[Step]:580/46625, [Train_loss]:123.168159\n",
      "[Step]:590/46625, [Train_loss]:120.352394\n",
      "[Step]:600/46625, [Train_loss]:119.757080\n",
      "[Step]:610/46625, [Train_loss]:122.902855\n",
      "[Step]:620/46625, [Train_loss]:120.257484\n",
      "[Step]:630/46625, [Train_loss]:124.834778\n",
      "[Step]:640/46625, [Train_loss]:118.557251\n",
      "[Step]:650/46625, [Train_loss]:120.286865\n",
      "[Step]:660/46625, [Train_loss]:113.229141\n",
      "[Step]:670/46625, [Train_loss]:117.570656\n",
      "[Step]:680/46625, [Train_loss]:108.488174\n",
      "[Step]:690/46625, [Train_loss]:113.111916\n",
      "[Step]:700/46625, [Train_loss]:112.194229\n",
      "[Step]:710/46625, [Train_loss]:122.708801\n",
      "[Step]:720/46625, [Train_loss]:126.154396\n",
      "[Step]:730/46625, [Train_loss]:118.613815\n",
      "[Step]:740/46625, [Train_loss]:116.642189\n",
      "[Step]:750/46625, [Train_loss]:113.643753\n",
      "[Step]:760/46625, [Train_loss]:113.361877\n",
      "[Step]:770/46625, [Train_loss]:104.471451\n",
      "[Step]:780/46625, [Train_loss]:115.767120\n",
      "[Step]:790/46625, [Train_loss]:120.211456\n",
      "[Step]:800/46625, [Train_loss]:122.219444\n",
      "[Step]:810/46625, [Train_loss]:117.788460\n",
      "[Step]:820/46625, [Train_loss]:122.177383\n",
      "[Step]:830/46625, [Train_loss]:117.863297\n",
      "[Step]:840/46625, [Train_loss]:112.845367\n",
      "[Step]:850/46625, [Train_loss]:113.683319\n",
      "[Step]:860/46625, [Train_loss]:112.204926\n",
      "[Step]:870/46625, [Train_loss]:109.740814\n",
      "[Step]:880/46625, [Train_loss]:111.939964\n",
      "[Step]:890/46625, [Train_loss]:111.020195\n",
      "[Step]:900/46625, [Train_loss]:116.465935\n",
      "[Step]:910/46625, [Train_loss]:114.560715\n",
      "[Step]:920/46625, [Train_loss]:111.485901\n",
      "[Step]:930/46625, [Train_loss]:116.339775\n",
      "[Step]:940/46625, [Train_loss]:110.316605\n",
      "[Step]:950/46625, [Train_loss]:109.892654\n",
      "[Step]:960/46625, [Train_loss]:100.975143\n",
      "[Step]:970/46625, [Train_loss]:112.009155\n",
      "[Step]:980/46625, [Train_loss]:106.496620\n",
      "[Step]:990/46625, [Train_loss]:113.682724\n",
      "[Step]:1000/46625, [Train_loss]:120.446236\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import config\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import legacy_seq2seq as seq2seq\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# 数据读取和生成训练用的词向量\n",
    "class DataGen():\n",
    "\tdef __init__(self, filepath, args):\n",
    "\t\tself.seq_length = args.seq_length\n",
    "\t\tself.batch_size = args.batch_size\n",
    "\t\twith open(filepath, encoding='utf-8') as f:\n",
    "\t\t\tself.text = f.read()\n",
    "\t\t\tself.text = self.text.replace('\\n', '')\n",
    "\t\tself.total_len = len(self.text)\n",
    "\t\tself.words = list(set(self.text))\n",
    "\t\tself.words.sort()\n",
    "\t\tself.vocabulary_size = len(self.words)\n",
    "\t\tself.char2idx_dict = {w: i for i, w in enumerate(self.words)}\n",
    "\t\tself.idx2char_dict = {i: w for i, w in enumerate(self.words)}\n",
    "\t\tself._pointer = 0\n",
    "\t\tself.save_metadata(args.metadata)\n",
    "\tdef char2idx(self, c):\n",
    "\t\tidx = self.char2idx_dict[c]\n",
    "\t\treturn idx\n",
    "\t\t# if idx:\n",
    "\t\t# \treturn idx\n",
    "\t\t# else:\n",
    "\t\t# \tself.vocabulary_size += 1\n",
    "\t\t# \tself.char2idx_dict[c] = self.vocabulary_size\n",
    "\t\t# \treturn self.vocabulary_size\n",
    "\tdef idx2char(self, idx):\n",
    "\t\treturn self.idx2char_dict[idx]\n",
    "\tdef save_metadata(self, filename):\n",
    "\t\twith open(filename, 'w', encoding='utf-8') as f:\n",
    "\t\t\tf.write('idx\\tchar\\n')\n",
    "\t\t\tfor i in range(self.vocabulary_size):\n",
    "\t\t\t\tc = self.idx2char(i)\n",
    "\t\t\t\tf.write('{}\\t{}\\n'.format(i, c))\n",
    "\tdef next_batch(self):\n",
    "\t\tx_batches = []\n",
    "\t\ty_batches = []\n",
    "\t\tfor i in range(self.batch_size):\n",
    "\t\t\tif self._pointer + self.seq_length + 1 >= self.total_len:\n",
    "\t\t\t\tself._pointer = 0\n",
    "\t\t\tbatch_x = self.text[self._pointer: self._pointer+self.seq_length]\n",
    "\t\t\tbatch_y = self.text[self._pointer+1: self._pointer+self.seq_length+1]\n",
    "\t\t\tself._pointer += self.seq_length\n",
    "\t\t\tbatch_x = [self.char2idx(c) for c in batch_x]\n",
    "\t\t\tbatch_y = [self.char2idx(c) for c in batch_y]\n",
    "\t\t\tx_batches.append(batch_x)\n",
    "\t\t\ty_batches.append(batch_y)\n",
    "\t\treturn x_batches, y_batches\n",
    "\n",
    "\n",
    "# rnn\n",
    "class Model():\n",
    "\tdef __init__(self, args, text, test=False):\n",
    "\t\tif test:\n",
    "\t\t\targs.batch_size = 1\n",
    "\t\t\targs.seq_length = 1\n",
    "\t\twith tf.name_scope('inputs'):\n",
    "\t\t\tself.input_text = tf.placeholder(\n",
    "\t\t\t\ttf.int32, [args.batch_size, args.seq_length])\n",
    "\t\t\tself.target_text = tf.placeholder(\n",
    "\t\t\t\ttf.int32, [args.batch_size, args.seq_length])\n",
    "\t\twith tf.name_scope('model'):\n",
    "\t\t\t# LSTM单元 state_size为隐藏层的大小\n",
    "\t\t\tself.cell = rnn.BasicLSTMCell(args.state_size)\n",
    "\t\t\tself.cells = rnn.MultiRNNCell([self.cell] * args.num_layers)\n",
    "\t\t\tself.initial_state = self.cells.zero_state(\n",
    "\t\t\t\targs.batch_size, tf.float32)\n",
    "\t\t\twith tf.variable_scope('rnnlm'):\n",
    "\t\t\t\tw = tf.get_variable('softmax_w', [args.state_size, text.vocabulary_size])\n",
    "\t\t\t\tb = tf.get_variable('softmax_b', [text.vocabulary_size])\n",
    "\t\t\t\twith tf.device('/cpu:0'):\n",
    "\t\t\t\t\tembedding = tf.get_variable(\n",
    "\t\t\t\t\t\t'embedding', [text.vocabulary_size, args.state_size])\n",
    "\t\t\t\t\tinputs = tf.nn.embedding_lookup(embedding, self.input_text)\n",
    "\t\t\toutputs, last_state = tf.nn.dynamic_rnn(\n",
    "\t\t\t\tself.cells, inputs, initial_state=self.initial_state)\n",
    "\t\twith tf.name_scope('loss'):\n",
    "\t\t\toutput = tf.reshape(outputs, [-1, args.state_size])\n",
    "\t\t\tself.logits = tf.matmul(output, w) + b\n",
    "\t\t\tself.probs = tf.nn.softmax(self.logits)\n",
    "\t\t\tself.last_state = last_state\n",
    "\t\t\ttargets = tf.reshape(self.target_text, [-1])\n",
    "\t\t\tloss = seq2seq.sequence_loss_by_example([self.logits],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t[targets],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t[tf.ones_like(targets, dtype=tf.float32)])\n",
    "\t\t\tself.loss_avg = tf.reduce_sum(loss) / args.batch_size\n",
    "\t\t\ttf.summary.scalar('loss', self.loss_avg)\n",
    "\t\twith tf.name_scope('optimize'):\n",
    "\t\t\tself.lr = tf.placeholder(tf.float32, [])\n",
    "\t\t\ttf.summary.scalar('learning_rate', self.lr)\n",
    "\t\t\toptimizer = tf.train.AdamOptimizer(self.lr)\n",
    "\t\t\ttvars = tf.trainable_variables()\n",
    "\t\t\tgrads = tf.gradients(self.loss_avg, tvars)\n",
    "\t\t\tfor g in grads:\n",
    "\t\t\t\ttf.summary.histogram(g.name, g)\n",
    "\t\t\tgrads, _ = tf.clip_by_global_norm(grads, args.grad_clip)\n",
    "\t\t\tself.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\t\t\tself.merged_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "# 训练\n",
    "def train(text, model, args):\n",
    "\twith tf.Session() as sess:\n",
    "\t\tsess.run(tf.global_variables_initializer())\n",
    "\t\tsaver = tf.train.Saver()\n",
    "\t\twriter = tf.summary.FileWriter(args.log_dir, sess.graph)\n",
    "\t\tpconfig = projector.ProjectorConfig()\n",
    "\t\tembed = pconfig.embeddings.add()\n",
    "\t\tembed.tensor_name = 'rnnlm/embedding:0'\n",
    "\t\tembed.metadata_path = args.metadata\n",
    "\t\tprojector.visualize_embeddings(writer, pconfig)\n",
    "\t\tmax_iter = args.n_epoch * \\\n",
    "\t\t\t\t\t(text.total_len // args.seq_length) // args.batch_size\n",
    "\t\tfor i in range(max_iter):\n",
    "\t\t\tlearning_rate = args.learning_rate * \\\n",
    "\t\t\t\t\t\t\t(args.decay_rate ** (i // args.decay_steps))\n",
    "\t\t\tx_batch, y_batch = text.next_batch()\n",
    "\t\t\tfeed_dict = {\n",
    "\t\t\t\tmodel.input_text: x_batch,\n",
    "\t\t\t\tmodel.target_text: y_batch,\n",
    "\t\t\t\tmodel.lr: learning_rate\n",
    "\t\t\t}\n",
    "\t\t\ttrain_loss, summary, _, _ = sess.run([model.loss_avg,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  model.merged_op,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  model.last_state,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  model.train_op],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  feed_dict)\n",
    "\t\t\tif i % 10 == 0:\n",
    "\t\t\t\twriter.add_summary(summary, global_step=i)\n",
    "\t\t\t\tprint('[Step]:{}/{}, [Train_loss]:{:4f}'.format(i, max_iter, train_loss))\n",
    "\t\t\tif i % 2000 == 0 or (i + 1) == max_iter:\n",
    "\t\t\t\tsaver.save(sess, os.path.join(args.log_dir, 'model.ckpt'), global_step=i)\n",
    "\n",
    "\n",
    "# 生成文本\n",
    "# prime用于预热\n",
    "def GenText(text, model, args, prime):\n",
    "\tsaver = tf.train.Saver()\n",
    "\twith tf.Session() as sess:\n",
    "\t\tckpt = tf.train.latest_checkpoint(args.log_dir)\n",
    "\t\tsaver.restore(sess, ckpt)\n",
    "\t\tstate = sess.run(model.cells.zero_state(1, tf.float32))\n",
    "\t\tfor word in prime[:-1]:\n",
    "\t\t\tx = np.zeros((1, 1))\n",
    "\t\t\tx[0, 0] = text.char2idx(word)\n",
    "\t\t\tfeed_dict = {model.input_text: x, model.initial_state: state}\n",
    "\t\t\tstate = sess.run(model.last_state, feed_dict)\n",
    "\t\tword = prime[-1]\n",
    "\t\tcontents = prime\n",
    "\t\tfor i in range(args.gen_num):\t\n",
    "\t\t\tx = np.zeros([1, 1])\n",
    "\t\t\tx[0, 0] = text.char2idx(word)\n",
    "\t\t\tfeed_dict = {model.input_text: x, model.initial_state: state}\n",
    "\t\t\tprobs, state = sess.run([model.probs, model.last_state], feed_dict)\n",
    "\t\t\tprob = probs[0]\n",
    "\t\t\tword = text.idx2char(np.argmax(prob))\n",
    "\t\t\ttime.sleep(0.5)\n",
    "\t\t\t# sys.stdout.flush()\n",
    "\t\t\t# print(word, end='')\n",
    "\t\t\tcontents += word\n",
    "\t\treturn contents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tprint('[INFO]:Char_RNN Example...')\n",
    "\tprint('[Author]:Charles')\n",
    "\tprint('[Usage]:python3 Char_RNN.py \\nEnter 1 to train, enter 0 to gen...')\n",
    "\tchoice = input('[Choice]:Train<1> or Gen<0>:')\n",
    "\ttext = DataGen('./JayLyrics.txt', config.HParam())\n",
    "\tprime = u'是曾与你躲过雨的屋檐'\n",
    "\tif choice == '1':\n",
    "\t\tmodel = Model(config.HParam(), text, False)\n",
    "\t\ttrain(text, model, config.HParam())\n",
    "\telif choice == '0':\n",
    "\t\tmodel = Model(config.HParam(), text, True)\n",
    "\t\tcontents = GenText(text, model, config.HParam(), prime)\n",
    "\t\twith open('./results.txt', 'w') as f:\n",
    "\t\t\tf.write(contents)\n",
    "\telse:\n",
    "\t\tprint('[Error]:Enter cannot recognized...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
