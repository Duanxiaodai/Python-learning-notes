{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda\n",
      "1\n",
      "[INFO]:Building the style transfer model...\n",
      "[INFO]:Optimizing...\n",
      "[RUN]: [50]\n",
      "[Style Loss]: 8.193624 [Content Loss]: 3.258350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:164: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN]: [100]\n",
      "[Style Loss]: 3.399277 [Content Loss]: 2.205480\n",
      "\n",
      "[RUN]: [150]\n",
      "[Style Loss]: 1.646805 [Content Loss]: 2.014344\n",
      "\n",
      "[RUN]: [200]\n",
      "[Style Loss]: 0.649033 [Content Loss]: 2.082869\n",
      "\n",
      "[RUN]: [250]\n",
      "[Style Loss]: 0.465811 [Content Loss]: 1.898731\n",
      "\n",
      "[RUN]: [300]\n",
      "[Style Loss]: 0.445368 [Content Loss]: 1.809600\n",
      "\n",
      "[RUN]: [350]\n",
      "[Style Loss]: 0.463533 [Content Loss]: 1.745905\n",
      "\n",
      "[RUN]: [400]\n",
      "[Style Loss]: 0.406535 [Content Loss]: 1.749085\n",
      "\n",
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npAFTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import copy\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('use_cuda')\n",
    "\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "\n",
    "\n",
    "# 载入图像\n",
    "imgsize = 256 if use_cuda else 128\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize(imgsize),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = Variable(loader(image))\n",
    "    # 添加假的batch维来满足网络输入\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "\n",
    "# 内容损失\n",
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target, weight):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # 分离目标内容\n",
    "        self.target = target.detach() * weight\n",
    "        # 为了动态计算梯度，它为一个定值而非变量\n",
    "        # 否则前向的标准将抛出异常\n",
    "        self.weight = weight\n",
    "        self.criterion = nn.MSELoss()\n",
    "    def forward(self, input_):\n",
    "        self.loss = self.criterion(input_ * self.weight, self.target)\n",
    "        self.output = input_\n",
    "        return self.output\n",
    "    def backward(self, retain_graph=True):\n",
    "        self.loss.backward(retain_graph=retain_graph)\n",
    "        return self.loss\n",
    "\n",
    "\n",
    "# 风格损失\n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, input_):\n",
    "        # a=1,是batch_size\n",
    "        # b为特征图数量\n",
    "        # (c, d)为f的维度(N=c*d)\n",
    "        a, b, c, d = input_.size()\n",
    "        features = input_.view(a * b, c * d)\n",
    "        G = torch.mm(features, features.t())\n",
    "        # 每个特征图除以元素数量来归一化gram矩阵\n",
    "        return G.div(a * b * c * d)\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target, weight):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = target.detach() * weight\n",
    "        self.weight = weight\n",
    "        self.gram = GramMatrix()\n",
    "        self.criterion = nn.MSELoss()\n",
    "    def forward(self, input_):\n",
    "        self.output = input_.clone()\n",
    "        self.G = self.gram(input_)\n",
    "        self.G.mul_(self.weight)\n",
    "        self.loss = self.criterion(self.G, self.target)\n",
    "        return self.output\n",
    "    def backward(self, retain_graph=True):\n",
    "        self.loss.backward(retain_graph=retain_graph)\n",
    "        return self.loss\n",
    "\n",
    "\n",
    "# 载入VGG\n",
    "cnn = models.vgg19(pretrained=True).features\n",
    "if use_cuda:\n",
    "    cnn = cnn.cuda()\n",
    "content_layers_default = ['conv_4']\n",
    "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "def get_style_model_and_losses(cnn, style_img, content_img,\n",
    "                               style_weight=1000, content_weight=1,\n",
    "                               content_layers=content_layers_default,\n",
    "                               style_layers=style_layers_default):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "    model = nn.Sequential()\n",
    "    gram = GramMatrix()\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        gram = gram.cuda()\n",
    "    i = 1\n",
    "    for layer in list(cnn):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            name = 'conv_' + str(i)\n",
    "            model.add_module(name, layer)\n",
    "            if name in content_layers:\n",
    "                target = model(content_img).clone()\n",
    "                content_loss = ContentLoss(target, content_weight)\n",
    "                model.add_module('content_loss_' + str(i), content_loss)\n",
    "                content_losses.append(content_loss)\n",
    "            if name in style_layers:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                style_loss = StyleLoss(target_feature_gram, style_weight)\n",
    "                model.add_module('style_loss_' + str(i), style_loss)\n",
    "                style_losses.append(style_loss)\n",
    "        if isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_' + str(i)\n",
    "            model.add_module(name, layer)\n",
    "            if name in content_layers:\n",
    "                target = model(content_img).clone()\n",
    "                content_loss = ContentLoss(target, content_weight)\n",
    "                model.add_module('content_loss_' + str(i), content_loss)\n",
    "                content_losses.append(content_loss)\n",
    "            if name in style_layers:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                style_loss = StyleLoss(target_feature_gram, style_weight)\n",
    "                model.add_module('style_loss_' + str(i), style_loss)\n",
    "                style_losses.append(style_loss)\n",
    "            i += 1\n",
    "        if isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_' + str(i)\n",
    "            model.add_module(name, layer)\n",
    "    return model, style_losses, content_losses\n",
    "\n",
    "\n",
    "# 梯度下降\n",
    "def get_input_param_optimizer(input_img):\n",
    "    input_param = nn.Parameter(input_img.data)\n",
    "    optimizer = optim.LBFGS([input_param])\n",
    "    return input_param, optimizer\n",
    "def run_style_transfer(cnn, content_img, style_img, input_img, num_steps=300,\n",
    "                       style_weight=1000, content_weight=1):\n",
    "    print('[INFO]:Building the style transfer model...')\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n",
    "                                            style_img, content_img, style_weight, content_weight)\n",
    "    input_param, optimizer = get_input_param_optimizer(input_img)\n",
    "    print('[INFO]:Optimizing...')\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "        def closure():\n",
    "            input_param.data.clamp_(0, 1)\n",
    "            optimizer.zero_grad()\n",
    "            model(input_param)\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "            for sl in style_losses:\n",
    "                style_score += sl.backward()\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.backward()\n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print('[RUN]: {}'.format(run))\n",
    "                print('[Style Loss]: {:4f} [Content Loss]: {:4f}'.format(\n",
    "                    style_score.data[0], content_score.data[0]\n",
    "                ))\n",
    "                print('')\n",
    "            return style_score + content_score\n",
    "        optimizer.step(closure)\n",
    "    input_param.data.clamp_(0, 1)\n",
    "    return input_param.data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 风格图像\n",
    "    style_img = image_loader('./images/sunflower.jpg').type(dtype)\n",
    "    # 内容图像\n",
    "    content_img = image_loader('./images/wife.jpg').type(dtype)\n",
    "    input_img = content_img.clone()\n",
    "    assert style_img.size() == content_img.size(), \\\n",
    "            \"we need to import style and content images of the same size\"\n",
    "    print('1')\n",
    "    output = run_style_transfer(cnn, content_img, style_img, input_img, num_steps=400)\n",
    "    print('2')\n",
    "    image = output.clone().cpu()\n",
    "    image = image.view(3, imgsize, imgsize)\n",
    "    unloader = transforms.ToPILImage()\n",
    "    image = unloader(image)\n",
    "    image.save('./result.jpg')\n",
    "    plt.show(image)\n",
    "    plt.title('Result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
