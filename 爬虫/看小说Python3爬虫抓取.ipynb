{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "if __name__ == \"__main__\":\n",
    "    target = 'http://www.biqukan.com//1_1094/5447905.html'\n",
    "    req = requests.get(url = target) \n",
    "    html = req.text\n",
    "    bf = BeautifulSoup(html,'html5lib')\n",
    "    texts = bf.find_all('div', id = 'content', class_ = 'showtxt')\n",
    "    print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# if __name__ == \"__main__\":\n",
    "#     server = 'http://www.biqukan.com'\n",
    "#     target = 'http://www.biqukan.com/1_1094/'\n",
    "#     req = requests.get(url = target) \n",
    "#     html = req.text\n",
    "#     div_bf = BeautifulSoup(html,'html5lib')\n",
    "#     div = div_bf.find_all('div', class_ = 'listmain')\n",
    "#     a_bf = BeautifulSoup(str(div[0]))\n",
    "#     a = a_bf.find_all('a')\n",
    "#     for each in a:\n",
    "#          print(each.string, server + each.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file d:\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "《一年永恒》开始下载：\n",
      "1342      1342     1342\n",
      "0\n",
      "0 http://www.biqukan.com/1_1094/5403177.html\n",
      "0\n",
      "1 http://www.biqukan.com/1_1094/5428081.html\n",
      "0\n",
      "2 http://www.biqukan.com/1_1094/5433843.html\n",
      "1\n",
      "3 第四章 炼灵 下载成功\n",
      "1\n",
      "4 第五章 万一丢了小命咋办 下载成功\n",
      "1\n",
      "5 第六章 灵气上头 下载成功\n",
      "0\n",
      "6 http://www.biqukan.com/1_1094/5468474.html\n",
      "0\n",
      "7 http://www.biqukan.com/1_1094/5472334.html\n",
      "0\n",
      "8 http://www.biqukan.com/1_1094/5480646.html\n",
      "0\n",
      "9 http://www.biqukan.com/1_1094/5485337.html\n",
      "0\n",
      "10 http://www.biqukan.com/1_1094/5514174.html\n",
      "0\n",
      "11 http://www.biqukan.com/1_1094/5523833.html\n",
      "0\n",
      "12 http://www.biqukan.com/1_1094/5546163.html\n",
      "0\n",
      "13 http://www.biqukan.com/1_1094/5552312.html\n",
      "0\n",
      "14 http://www.biqukan.com/1_1094/5571525.html\n",
      "0\n",
      "15 http://www.biqukan.com/1_1094/5576469.html\n",
      "0\n",
      "16 http://www.biqukan.com/1_1094/5590802.html\n",
      "0\n",
      "17 http://www.biqukan.com/1_1094/5593773.html\n",
      "0\n",
      "18 http://www.biqukan.com/1_1094/5608623.html\n",
      "0\n",
      "19 http://www.biqukan.com/1_1094/5611260.html\n",
      "0\n",
      "20 http://www.biqukan.com/1_1094/5621379.html\n",
      "0\n",
      "21 http://www.biqukan.com/1_1094/5624052.html\n",
      "0\n",
      "22 http://www.biqukan.com/1_1094/5628174.html\n",
      "0\n",
      "23 http://www.biqukan.com/1_1094/5633005.html\n",
      "0\n",
      "24 http://www.biqukan.com/1_1094/5638811.html\n",
      "0\n",
      "25 http://www.biqukan.com/1_1094/5647433.html\n",
      "0\n",
      "26 http://www.biqukan.com/1_1094/5652658.html\n",
      "0\n",
      "27 http://www.biqukan.com/1_1094/5662778.html\n",
      "0\n",
      "28 http://www.biqukan.com/1_1094/5665335.html\n",
      "0\n",
      "29 http://www.biqukan.com/1_1094/5676615.html\n",
      "0\n",
      "30 http://www.biqukan.com/1_1094/5679107.html\n",
      "0\n",
      "31 http://www.biqukan.com/1_1094/5684578.html\n",
      "0\n",
      "32 http://www.biqukan.com/1_1094/5686552.html\n",
      "0\n",
      "33 http://www.biqukan.com/1_1094/5696066.html\n",
      "0\n",
      "34 http://www.biqukan.com/1_1094/5696432.html\n",
      "0\n",
      "35 http://www.biqukan.com/1_1094/5698212.html\n",
      "0\n",
      "36 http://www.biqukan.com/1_1094/5700352.html\n",
      "0\n",
      "37 http://www.biqukan.com/1_1094/5702552.html\n",
      "0\n",
      "38 http://www.biqukan.com/1_1094/5706196.html\n",
      "0\n",
      "39 http://www.biqukan.com/1_1094/5708227.html\n",
      "0\n",
      "40 http://www.biqukan.com/1_1094/5715301.html\n",
      "0\n",
      "41 http://www.biqukan.com/1_1094/5716953.html\n",
      "0\n",
      "42 http://www.biqukan.com/1_1094/5751738.html\n",
      "0\n",
      "43 http://www.biqukan.com/1_1094/5771357.html\n",
      "0\n",
      "44 http://www.biqukan.com/1_1094/5814894.html\n",
      "0\n",
      "45 http://www.biqukan.com/1_1094/5822748.html\n",
      "0\n",
      "46 http://www.biqukan.com/1_1094/5849243.html\n",
      "0\n",
      "47 http://www.biqukan.com/1_1094/5855788.html\n",
      "0\n",
      "48 http://www.biqukan.com/1_1094/5873532.html\n",
      "0\n",
      "49 http://www.biqukan.com/1_1094/5882909.html\n",
      "0\n",
      "50 http://www.biqukan.com/1_1094/5899569.html\n",
      "0\n",
      "51 http://www.biqukan.com/1_1094/5904311.html\n",
      "0\n",
      "52 http://www.biqukan.com/1_1094/5910295.html\n",
      "0\n",
      "53 http://www.biqukan.com/1_1094/5922361.html\n",
      "0\n",
      "54 http://www.biqukan.com/1_1094/5926235.html\n",
      "0\n",
      "55 http://www.biqukan.com/1_1094/5936334.html\n",
      "0\n",
      "56 http://www.biqukan.com/1_1094/5941123.html\n",
      "0\n",
      "57 http://www.biqukan.com/1_1094/5953553.html\n",
      "0\n",
      "58 http://www.biqukan.com/1_1094/5956603.html\n",
      "0\n",
      "59 http://www.biqukan.com/1_1094/5966965.html\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, sys\n",
    "\n",
    "\"\"\"\n",
    "类说明:下载《笔趣看》网小说《一念永恒》\n",
    "Parameters:\n",
    "    无\n",
    "Returns:\n",
    "    无\n",
    "\n",
    "\"\"\"\n",
    "class downloader(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.server = 'http://www.biqukan.com'\n",
    "        self.target = 'http://www.biqukan.com/1_1094/'\n",
    "        self.names = []            #存放章节名\n",
    "        self.urls = []            #存放章节链接\n",
    "        self.nums = 0            #章节数\n",
    "\n",
    "    \"\"\"\n",
    "    函数说明:获取下载链接\n",
    "    Parameters:\n",
    "        无\n",
    "    Returns:\n",
    "        无\n",
    "\n",
    "    \"\"\"\n",
    "    def get_download_url(self):\n",
    "        req = requests.get(url = self.target)\n",
    "        html = req.text\n",
    "        div_bf = BeautifulSoup(html,'html5lib')\n",
    "        div = div_bf.find_all('div', class_ = 'listmain')\n",
    "        a_bf = BeautifulSoup(str(div[0]))\n",
    "        a = a_bf.find_all('a')\n",
    "        self.nums = len(a[15:])                                #剔除不必要的章节，并统计章节数\n",
    "        for each in a[15:]:\n",
    "            self.names.append(each.string)\n",
    "            self.urls.append(self.server + each.get('href'))\n",
    "\n",
    "    \"\"\"\n",
    "    函数说明:获取章节内容\n",
    "    Parameters:\n",
    "        target - 下载连接(string)\n",
    "    Returns:\n",
    "        texts - 章节内容(string)\n",
    "\n",
    "    \"\"\"\n",
    "    def get_contents(self, target):\n",
    "        req = requests.get(url = target)\n",
    "        \n",
    "        html = req.text\n",
    "        bf = BeautifulSoup(html,'html5lib')\n",
    "        texts = bf.find_all('div', class_ = 'showtxt')\n",
    "        print(len(texts))\n",
    "        texts = texts[0].text.replace('\\xa0'*8,'\\n\\n')\n",
    "        \n",
    "        return texts\n",
    "\n",
    "    \"\"\"\n",
    "    函数说明:将爬取的文章内容写入文件\n",
    "    Parameters:\n",
    "        name - 章节名称(string)\n",
    "        path - 当前路径下,小说保存名称(string)\n",
    "        text - 章节内容(string)\n",
    "    Returns:\n",
    "        无\n",
    "\n",
    "    \"\"\"\n",
    "    def writer(self, name, path, text):\n",
    "        write_flag = True\n",
    "        with open(path, 'a', encoding='utf-8') as f:\n",
    "            f.write(name + '\\n')\n",
    "            f.writelines(text)\n",
    "            f.write('\\n\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dl = downloader()\n",
    "    dl.get_download_url()\n",
    "    print('《一年永恒》开始下载：')\n",
    "    print(dl.nums,'    ',len(dl.names),'   ',len(dl.urls))\n",
    "    for i in range(dl.nums):\n",
    "        try:\n",
    "            \n",
    "            dl.writer(dl.names[i], '一念永恒.txt', dl.get_contents(dl.urls[i]))\n",
    "            sys.stdout.write(\"  已下载:%.3f%%\" %  float(i/dl.nums) + '\\r')\n",
    "            sys.stdout.flush()\n",
    "            print(i,dl.names[i],'下载成功')\n",
    "        except :\n",
    "            print(i,dl.urls[i])\n",
    "    print('《一年永恒》下载完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
